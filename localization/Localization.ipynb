{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Localization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucaskawazoi/lkk-xview2/blob/master/localization/Localization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii0fxyLbKXlY",
        "colab_type": "text"
      },
      "source": [
        "https://www.xview2.org/challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsytk27TP5Mu",
        "colab_type": "text"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbp03VK0MClb",
        "colab_type": "code",
        "outputId": "49862e6a-17d5-4518-9457-f66502078c7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "print('TF version:', tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "TF version: 1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxS14oKPlMF8",
        "colab_type": "text"
      },
      "source": [
        "# flags main.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lLtsc01l4Vw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flags = tf.app.flags"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i-qj8cdLloli",
        "colab": {}
      },
      "source": [
        "flags.DEFINE_string('f', '', 'kernel')\n",
        "\n",
        "# Cloud TPU Cluster Resolver flags\n",
        "flags.DEFINE_string(\n",
        "    \"tpu\", default=None,\n",
        "    help=\"The Cloud TPU to use for training. This should be either the name \"\n",
        "    \"used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 \"\n",
        "    \"url.\")\n",
        "flags.DEFINE_string(\n",
        "    \"tpu_zone\", default=None,\n",
        "    help=\"[Optional] GCE zone where the Cloud TPU is located in. If not \"\n",
        "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
        "    \"metadata.\")\n",
        "flags.DEFINE_string(\n",
        "    \"gcp_project\", default=None,\n",
        "    help=\"[Optional] Project name for the Cloud TPU-enabled project. If not \"\n",
        "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
        "    \"metadata.\")\n",
        "\n",
        "# Model specific parameters\n",
        "flags.DEFINE_string(\"data_dir\", \"\",\n",
        "                    \"Path to directory containing the MNIST dataset\")\n",
        "flags.DEFINE_string(\"model_dir\", None, \"Estimator model_dir\")\n",
        "flags.DEFINE_integer(\"batch_size\", 1024,\n",
        "                     \"Mini-batch size for the training. Note that this \"\n",
        "                     \"is the global batch size and not the per-shard batch.\")\n",
        "flags.DEFINE_integer(\"train_steps\", 1000, \"Total number of training steps.\")\n",
        "flags.DEFINE_integer(\"eval_steps\", 0,\n",
        "                     \"Total number of evaluation steps. If `0`, evaluation \"\n",
        "                     \"after training is skipped.\")\n",
        "flags.DEFINE_float(\"learning_rate\", 0.05, \"Learning rate.\")\n",
        "\n",
        "flags.DEFINE_bool(\"use_tpu\", True, \"Use TPUs rather than plain CPUs\")\n",
        "flags.DEFINE_bool(\"enable_predict\", True, \"Do some predictions at the end\")\n",
        "flags.DEFINE_integer(\"iterations\", 50,\n",
        "                     \"Number of iterations per TPU training loop.\")\n",
        "flags.DEFINE_integer(\"num_shards\", 8, \"Number of shards (TPU chips).\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iETWJMmgl-9b",
        "colab_type": "code",
        "outputId": "51e72d00-bf8f-42eb-a4b1-bf6fa20c37f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "FLAGS = flags.FLAGS\n",
        "FLAGS"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<absl.flags._flagvalues.FlagValues at 0x7fb0072c47f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7Yn-P_wP0FN",
        "colab_type": "text"
      },
      "source": [
        "# train_dataset\n",
        "\n",
        "    train_dataset = dataset.Dataset(\n",
        "        data_sources=file_pattern,\n",
        "        reader=tf.TFRecordReader,\n",
        "        decoder=decoder,\n",
        "        num_samples=splits_to_sizes[split_name],\n",
        "        items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n",
        "        ignore_label=ignore_label,\n",
        "        num_classes=num_classes,\n",
        "        name=dataset_name,\n",
        "        multi_label=True)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LycsfcaNpff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.contrib import slim as contrib_slim\n",
        "slim = contrib_slim\n",
        "dataset = slim.dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmAhgcvQMCMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Variables\n",
        "dataset_name = 'xview2'\n",
        "split_name = 'train'\n",
        "dataset_dir = 'gs://lkk-xview2/xBD/spacenet_gt/images'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpvJ8yyKN6Mi",
        "colab_type": "code",
        "outputId": "cc97bdca-a45c-49f8-b6c7-1004438fab06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# file_pattern\n",
        "import os\n",
        "_FILE_PATTERN = '%s-*'\n",
        "file_pattern = _FILE_PATTERN\n",
        "file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n",
        "file_pattern"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'gs://lkk-xview2/xBD/spacenet_gt/images/train-*'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ag48OyLmOKJ2",
        "colab_type": "code",
        "outputId": "01affae5-417a-40f0-d4d5-3c3146e2be58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# decoder\n",
        "tfexample_decoder = slim.tfexample_decoder\n",
        "\n",
        "keys_to_features = {\n",
        "      'image/encoded': tf.FixedLenFeature(\n",
        "          (), tf.string, default_value=''),\n",
        "      'image/filename': tf.FixedLenFeature(\n",
        "          (), tf.string, default_value=''),\n",
        "      'image/format': tf.FixedLenFeature(\n",
        "          (), tf.string, default_value='jpeg'),\n",
        "      'image/height': tf.FixedLenFeature(\n",
        "          (), tf.int64, default_value=0),\n",
        "      'image/width': tf.FixedLenFeature(\n",
        "          (), tf.int64, default_value=0),\n",
        "      'image/segmentation/class/encoded': tf.FixedLenFeature(\n",
        "          (), tf.string, default_value=''),\n",
        "      'image/segmentation/class/format': tf.FixedLenFeature(\n",
        "          (), tf.string, default_value='png'),\n",
        "  }\n",
        "\n",
        "items_to_handlers = {\n",
        "      'image': tfexample_decoder.Image(\n",
        "          image_key='image/encoded',\n",
        "          format_key='image/format',\n",
        "          channels=3),\n",
        "      'image_name': tfexample_decoder.Tensor('image/filename'),\n",
        "      'height': tfexample_decoder.Tensor('image/height'),\n",
        "      'width': tfexample_decoder.Tensor('image/width'),\n",
        "      'labels_class': tfexample_decoder.Image(\n",
        "          image_key='image/segmentation/class/encoded',\n",
        "          format_key='image/segmentation/class/format',\n",
        "          channels=1),\n",
        "  }\n",
        "\n",
        "decoder = tfexample_decoder.TFExampleDecoder(\n",
        "      keys_to_features, items_to_handlers)\n",
        "decoder"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.contrib.slim.python.slim.data.tfexample_decoder.TFExampleDecoder at 0x7f02cbfa6c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlmnC2FIOiCF",
        "colab_type": "code",
        "outputId": "ae1cdcd0-8a8b-4ca5-d93f-e23bb0396135",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# split_to_sizes\n",
        "\n",
        "import collections\n",
        "\n",
        "# Named tuple to describe the dataset properties.\n",
        "DatasetDescriptor = collections.namedtuple(\n",
        "    'DatasetDescriptor',\n",
        "    ['splits_to_sizes',   # Splits of the dataset into training, val, and test.\n",
        "     'num_classes',   # Number of semantic classes, including the background\n",
        "                      # class (if exists). For example, there are 20\n",
        "                      # foreground classes + 1 background class in the PASCAL\n",
        "                      # VOC 2012 dataset. Thus, we set num_classes=21.\n",
        "     'ignore_label',  # Ignore label value.\n",
        "    ]\n",
        ")\n",
        "\n",
        "_XVIEW2_INFORMATION = DatasetDescriptor(\n",
        "    splits_to_sizes={\n",
        "        'train': 1712,\n",
        "        'val': 571,\n",
        "    },\n",
        "    num_classes=2,\n",
        "    ignore_label=255,\n",
        ")\n",
        "\n",
        "_DATASETS_INFORMATION = {\n",
        "    'xview2': _XVIEW2_INFORMATION\n",
        "    }\n",
        "\n",
        "splits_to_sizes = _DATASETS_INFORMATION[dataset_name].splits_to_sizes\n",
        "splits_to_sizes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': 1712, 'val': 571}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54qMziFQPXtJ",
        "colab_type": "code",
        "outputId": "7c11a761-a50d-4cc4-b0b1-0ba608717588",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# _ITEMS_TO_DESCRIPTIONS\n",
        "_ITEMS_TO_DESCRIPTIONS = {\n",
        "    'image': 'A color image of varying height and width.',\n",
        "    'labels_class': ('A semantic segmentation label whose size matches image.'\n",
        "                     'Its values range from 0 (background) to num_classes.'),\n",
        "}\n",
        "_ITEMS_TO_DESCRIPTIONS"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'image': 'A color image of varying height and width.',\n",
              " 'labels_class': 'A semantic segmentation label whose size matches image.Its values range from 0 (background) to num_classes.'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LO37wxvKPltt",
        "colab_type": "code",
        "outputId": "2a1e3b0b-baf9-4901-89a8-7b584b0adfc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# ignore_label\n",
        "ignore_label = _DATASETS_INFORMATION[dataset_name].ignore_label\n",
        "ignore_label"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "255"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49_5675pPrtM",
        "colab_type": "code",
        "outputId": "68d8110a-a97f-43f6-a1b9-dd8b53129d59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# num_classes\n",
        "num_classes = _DATASETS_INFORMATION[dataset_name].num_classes\n",
        "num_classes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQF1xZfPLUx3",
        "colab_type": "code",
        "outputId": "b6d9878f-c987-40ed-8efe-f13ed40dc7af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_dataset = dataset.Dataset(\n",
        "    data_sources=file_pattern,\n",
        "    reader=tf.TFRecordReader,\n",
        "    decoder=decoder,\n",
        "    num_samples=splits_to_sizes[split_name],\n",
        "    items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n",
        "    ignore_label=ignore_label,\n",
        "    num_classes=num_classes,\n",
        "    name=dataset_name,\n",
        "    multi_label=True)\n",
        "train_dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.contrib.slim.python.slim.data.dataset.Dataset at 0x7f02cbf93e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTF3coL_QJPS",
        "colab_type": "text"
      },
      "source": [
        "# Eval dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loYdzfxTQ0_o",
        "colab_type": "code",
        "outputId": "a9ad9bac-9ce4-46ee-83b9-8107fe4dd141",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# file_pattern\n",
        "split_name = 'val'\n",
        "file_pattern = _FILE_PATTERN\n",
        "file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n",
        "file_pattern"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'gs://lkk-xview2/xBD/spacenet_gt/images/val-*'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7cF_qntQga9",
        "colab_type": "code",
        "outputId": "8ec268ce-b39a-419f-ceb1-abe793b05b21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "split_name = 'val'\n",
        "eval_dataset = dataset.Dataset(\n",
        "    data_sources=file_pattern,\n",
        "    reader=tf.TFRecordReader,\n",
        "    decoder=decoder,\n",
        "    num_samples=splits_to_sizes[split_name],\n",
        "    items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n",
        "    ignore_label=ignore_label,\n",
        "    num_classes=num_classes,\n",
        "    name=dataset_name,\n",
        "    multi_label=True)\n",
        "\n",
        "eval_dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.contrib.slim.python.slim.data.dataset.Dataset at 0x7f02d9100a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtggpuT-ReoO",
        "colab_type": "code",
        "outputId": "e95413c2-092d-4826-8c30-e2618d840c34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# batch size\n",
        "train_batch_size = 64\n",
        "\n",
        "num_train_images = train_dataset.num_samples\n",
        "num_classes = train_dataset.num_classes\n",
        "ignore_label = train_dataset.ignore_label\n",
        "num_batches_per_epoch = num_train_images / train_batch_size\n",
        "\n",
        "print(num_train_images, num_classes, ignore_label, num_batches_per_epoch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1712 2 255 26.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwTsnXgCRo45",
        "colab_type": "text"
      },
      "source": [
        "# Config TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIJJXJqoWjmM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Config credentials to run own TPU\n",
        "# !gsutil cp gs://[JSON_FILE] gcloud_key_file.json\n",
        "import os\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/gcloud_key_file.json\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvwZt3AC4YWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tpu_cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n",
        "      FLAGS.tpu if (FLAGS.tpu or FLAGS.use_tpu) else \"\",\n",
        "      zone=FLAGS.tpu_zone,\n",
        "      project=FLAGS.gcp_project\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5kjCfKFRp7N",
        "colab_type": "code",
        "outputId": "6fc0e1a3-2946-45c6-e595-0e231850c81f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu=\"lucas\",\n",
        "    zone='us-central1-f',\n",
        "    project='lkk-project-1')\n",
        "tpu_cluster_resolver"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver at 0x7f02cbf93a20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8WUgd80Rv6G",
        "colab_type": "code",
        "outputId": "0dc6ca76-e649-487e-c2d2-642dc5885aad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "config = tf.contrib.tpu.RunConfig(\n",
        "    cluster=tpu_cluster_resolver,\n",
        "    model_dir=None,\n",
        "    save_checkpoints_steps=2000,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=2000,\n",
        "        num_shards=8))\n",
        "config"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow_estimator.python.estimator.tpu.tpu_config.RunConfig at 0x7f02cbfa6b38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI2rmlDsmmB6",
        "colab_type": "text"
      },
      "source": [
        "# flags common"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EI3UOAsrm2N6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flags_common = tf.app.flags"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7K3Ya7oml4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Flags for input preprocessing.\n",
        "\n",
        "flags_common.DEFINE_integer('min_resize_value', None,\n",
        "                     'Desired size of the smaller image side.')\n",
        "\n",
        "flags_common.DEFINE_integer('max_resize_value', None,\n",
        "                     'Maximum allowed size of the larger image side.')\n",
        "\n",
        "flags_common.DEFINE_integer('resize_factor', None,\n",
        "                     'Resized dimensions are multiple of factor plus one.')\n",
        "\n",
        "flags_common.DEFINE_boolean('keep_aspect_ratio', True,\n",
        "                     'Keep aspect ratio after resizing or not.')\n",
        "\n",
        "# Model dependent flags.\n",
        "\n",
        "flags_common.DEFINE_integer('logits_kernel_size', 1,\n",
        "                     'The kernel size for the convolutional kernel that '\n",
        "                     'generates logits.')\n",
        "\n",
        "# When using 'mobilent_v2', we set atrous_rates = decoder_output_stride = None.\n",
        "# When using 'xception_65' or 'resnet_v1' model variants, we set\n",
        "# atrous_rates = [6, 12, 18] (output stride 16) and decoder_output_stride = 4.\n",
        "# See core/feature_extractor.py for supported model variants.\n",
        "flags_common.DEFINE_string('model_variant', 'mobilenet_v2', 'DeepLab model variant.')\n",
        "\n",
        "flags_common.DEFINE_multi_float('image_pyramid', None,\n",
        "                         'Input scales for multi-scale feature extraction.')\n",
        "\n",
        "flags_common.DEFINE_boolean('add_image_level_feature', True,\n",
        "                     'Add image level feature.')\n",
        "\n",
        "flags_common.DEFINE_list(\n",
        "    'image_pooling_crop_size', None,\n",
        "    'Image pooling crop size [height, width] used in the ASPP module. When '\n",
        "    'value is None, the model performs image pooling with \"crop_size\". This'\n",
        "    'flag is useful when one likes to use different image pooling sizes.')\n",
        "\n",
        "flags_common.DEFINE_list(\n",
        "    'image_pooling_stride', '1,1',\n",
        "    'Image pooling stride [height, width] used in the ASPP image pooling. ')\n",
        "\n",
        "flags_common.DEFINE_boolean('aspp_with_batch_norm', True,\n",
        "                     'Use batch norm parameters for ASPP or not.')\n",
        "\n",
        "flags_common.DEFINE_boolean('aspp_with_separable_conv', True,\n",
        "                     'Use separable convolution for ASPP or not.')\n",
        "\n",
        "# Defaults to None. Set multi_grid = [1, 2, 4] when using provided\n",
        "# 'resnet_v1_{50,101}_beta' checkpoints.\n",
        "flags_common.DEFINE_multi_integer('multi_grid', None,\n",
        "                           'Employ a hierarchy of atrous rates for ResNet.')\n",
        "\n",
        "flags_common.DEFINE_float('depth_multiplier', 1.0,\n",
        "                   'Multiplier for the depth (number of channels) for all '\n",
        "                   'convolution ops used in MobileNet.')\n",
        "\n",
        "flags_common.DEFINE_integer('divisible_by', None,\n",
        "                     'An integer that ensures the layer # channels are '\n",
        "                     'divisible by this value. Used in MobileNet.')\n",
        "\n",
        "# For `xception_65`, use decoder_output_stride = 4. For `mobilenet_v2`, use\n",
        "# decoder_output_stride = None.\n",
        "flags_common.DEFINE_list('decoder_output_stride', None,\n",
        "                  'Comma-separated list of strings with the number specifying '\n",
        "                  'output stride of low-level features at each network level.'\n",
        "                  'Current semantic segmentation implementation assumes at '\n",
        "                  'most one output stride (i.e., either None or a list with '\n",
        "                  'only one element.')\n",
        "\n",
        "flags_common.DEFINE_boolean('decoder_use_separable_conv', True,\n",
        "                     'Employ separable convolution for decoder or not.')\n",
        "\n",
        "flags_common.DEFINE_enum('merge_method', 'max', ['max', 'avg'],\n",
        "                  'Scheme to merge multi scale features.')\n",
        "\n",
        "flags_common.DEFINE_boolean(\n",
        "    'prediction_with_upsampled_logits', True,\n",
        "    'When performing prediction, there are two options: (1) bilinear '\n",
        "    'upsampling the logits followed by softmax, or (2) softmax followed by '\n",
        "    'bilinear upsampling.')\n",
        "\n",
        "flags_common.DEFINE_string(\n",
        "    'dense_prediction_cell_json',\n",
        "    '',\n",
        "    'A JSON file that specifies the dense prediction cell.')\n",
        "\n",
        "flags_common.DEFINE_integer(\n",
        "    'nas_stem_output_num_conv_filters', 20,\n",
        "    'Number of filters of the stem output tensor in NAS models.')\n",
        "\n",
        "flags_common.DEFINE_bool('nas_use_classification_head', False,\n",
        "                  'Use image classification head for NAS model variants.')\n",
        "\n",
        "flags_common.DEFINE_bool('nas_remove_os32_stride', False,\n",
        "                  'Remove the stride in the output stride 32 branch.')\n",
        "\n",
        "flags_common.DEFINE_bool('use_bounded_activation', False,\n",
        "                  'Whether or not to use bounded activations. Bounded '\n",
        "                  'activations better lend themselves to quantized inference.')\n",
        "\n",
        "flags_common.DEFINE_boolean('aspp_with_concat_projection', True,\n",
        "                     'ASPP with concat projection.')\n",
        "\n",
        "flags_common.DEFINE_boolean('aspp_with_squeeze_and_excitation', False,\n",
        "                     'ASPP with squeeze and excitation.')\n",
        "\n",
        "flags_common.DEFINE_integer('aspp_convs_filters', 256, 'ASPP convolution filters.')\n",
        "\n",
        "flags_common.DEFINE_boolean('decoder_use_sum_merge', False,\n",
        "                     'Decoder uses simply sum merge.')\n",
        "\n",
        "flags_common.DEFINE_integer('decoder_filters', 256, 'Decoder filters.')\n",
        "\n",
        "flags_common.DEFINE_boolean('decoder_output_is_logits', False,\n",
        "                     'Use decoder output as logits or not.')\n",
        "\n",
        "flags_common.DEFINE_boolean('image_se_uses_qsigmoid', False, 'Use q-sigmoid.')\n",
        "\n",
        "flags_common.DEFINE_multi_float(\n",
        "    'label_weights', None,\n",
        "    'A list of label weights, each element represents the weight for the label '\n",
        "    'of its index, for example, label_weights = [0.1, 0.5] means the weight '\n",
        "    'for label 0 is 0.1 and the weight for label 1 is 0.5. If set as None, all '\n",
        "    'the labels have the same weight 1.0.')\n",
        "\n",
        "flags_common.DEFINE_float('batch_norm_decay', 0.9997, 'Batchnorm decay.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNbZPvCgm5Q1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FLAGS_COMMON = flags_common.FLAGS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5XV4wTFkqDR",
        "colab_type": "text"
      },
      "source": [
        "# Common.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqqdhJARkpz-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Constants\n",
        "\n",
        "# Perform semantic segmentation predictions.\n",
        "OUTPUT_TYPE = 'semantic'\n",
        "\n",
        "# Semantic segmentation item names.\n",
        "LABELS_CLASS = 'labels_class'\n",
        "IMAGE = 'image'\n",
        "HEIGHT = 'height'\n",
        "WIDTH = 'width'\n",
        "IMAGE_NAME = 'image_name'\n",
        "LABEL = 'label'\n",
        "ORIGINAL_IMAGE = 'original_image'\n",
        "\n",
        "# Test set name.\n",
        "TEST_SET = 'test'\n",
        "\n",
        "\n",
        "class ModelOptions(\n",
        "    collections.namedtuple('ModelOptions', [\n",
        "        'outputs_to_num_classes',\n",
        "        'crop_size',\n",
        "        'atrous_rates',\n",
        "        'output_stride',\n",
        "        'preprocessed_images_dtype',\n",
        "        'merge_method',\n",
        "        'add_image_level_feature',\n",
        "        'image_pooling_crop_size',\n",
        "        'image_pooling_stride',\n",
        "        'aspp_with_batch_norm',\n",
        "        'aspp_with_separable_conv',\n",
        "        'multi_grid',\n",
        "        'decoder_output_stride',\n",
        "        'decoder_use_separable_conv',\n",
        "        'logits_kernel_size',\n",
        "        'model_variant',\n",
        "        'depth_multiplier',\n",
        "        'divisible_by',\n",
        "        'prediction_with_upsampled_logits',\n",
        "        'dense_prediction_cell_config',\n",
        "        'nas_architecture_options',\n",
        "        'use_bounded_activation',\n",
        "        'aspp_with_concat_projection',\n",
        "        'aspp_with_squeeze_and_excitation',\n",
        "        'aspp_convs_filters',\n",
        "        'decoder_use_sum_merge',\n",
        "        'decoder_filters',\n",
        "        'decoder_output_is_logits',\n",
        "        'image_se_uses_qsigmoid',\n",
        "        'label_weights',\n",
        "        'sync_batch_norm_method',\n",
        "        'batch_norm_decay',\n",
        "    ])):\n",
        "  \"\"\"Immutable class to hold model options.\"\"\"\n",
        "\n",
        "  __slots__ = ()\n",
        "\n",
        "  def __new__(cls,\n",
        "              outputs_to_num_classes,\n",
        "              crop_size=None,\n",
        "              atrous_rates=None,\n",
        "              output_stride=8,\n",
        "              preprocessed_images_dtype=tf.float32):\n",
        "    \"\"\"Constructor to set default values.\n",
        "\n",
        "    Args:\n",
        "      outputs_to_num_classes: A dictionary from output type to the number of\n",
        "        classes. For example, for the task of semantic segmentation with 21\n",
        "        semantic classes, we would have outputs_to_num_classes['semantic'] = 21.\n",
        "      crop_size: A tuple [crop_height, crop_width].\n",
        "      atrous_rates: A list of atrous convolution rates for ASPP.\n",
        "      output_stride: The ratio of input to output spatial resolution.\n",
        "      preprocessed_images_dtype: The type after the preprocessing function.\n",
        "\n",
        "    Returns:\n",
        "      A new ModelOptions instance.\n",
        "    \"\"\"\n",
        "    dense_prediction_cell_config = None\n",
        "    if FLAGS_COMMON.dense_prediction_cell_json:\n",
        "      with tf.gfile.Open(FLAGS_COMMON.dense_prediction_cell_json, 'r') as f:\n",
        "        dense_prediction_cell_config = json.load(f)\n",
        "    decoder_output_stride = None\n",
        "    if FLAGS_COMMON.decoder_output_stride:\n",
        "      decoder_output_stride = [\n",
        "          int(x) for x in FLAGS_COMMON.decoder_output_stride]\n",
        "      if sorted(decoder_output_stride, reverse=True) != decoder_output_stride:\n",
        "        raise ValueError('Decoder output stride need to be sorted in the '\n",
        "                         'descending order.')\n",
        "    image_pooling_crop_size = None\n",
        "    if FLAGS_COMMON.image_pooling_crop_size:\n",
        "      image_pooling_crop_size = [int(x) for x in FLAGS_COMMON.image_pooling_crop_size]\n",
        "    image_pooling_stride = [1, 1]\n",
        "    if FLAGS_COMMON.image_pooling_stride:\n",
        "      image_pooling_stride = [int(x) for x in FLAGS_COMMON.image_pooling_stride]\n",
        "    label_weights = FLAGS_COMMON.label_weights\n",
        "    if label_weights is None:\n",
        "      label_weights = 1.0\n",
        "    nas_architecture_options = {\n",
        "        'nas_stem_output_num_conv_filters': (\n",
        "            FLAGS_COMMON.nas_stem_output_num_conv_filters),\n",
        "        'nas_use_classification_head': FLAGS_COMMON.nas_use_classification_head,\n",
        "        'nas_remove_os32_stride': FLAGS_COMMON.nas_remove_os32_stride,\n",
        "    }\n",
        "    return super(ModelOptions, cls).__new__(\n",
        "        cls, outputs_to_num_classes, crop_size, atrous_rates, output_stride,\n",
        "        preprocessed_images_dtype,\n",
        "        FLAGS_COMMON.merge_method,\n",
        "        FLAGS_COMMON.add_image_level_feature,\n",
        "        image_pooling_crop_size,\n",
        "        image_pooling_stride,\n",
        "        FLAGS_COMMON.aspp_with_batch_norm,\n",
        "        FLAGS_COMMON.aspp_with_separable_conv,\n",
        "        FLAGS_COMMON.multi_grid,\n",
        "        decoder_output_stride,\n",
        "        FLAGS_COMMON.decoder_use_separable_conv,\n",
        "        FLAGS_COMMON.logits_kernel_size,\n",
        "        FLAGS_COMMON.model_variant,\n",
        "        FLAGS_COMMON.depth_multiplier,\n",
        "        FLAGS_COMMON.divisible_by,\n",
        "        FLAGS_COMMON.prediction_with_upsampled_logits,\n",
        "        dense_prediction_cell_config,\n",
        "        nas_architecture_options,\n",
        "        FLAGS_COMMON.use_bounded_activation,\n",
        "        FLAGS_COMMON.aspp_with_concat_projection,\n",
        "        FLAGS_COMMON.aspp_with_squeeze_and_excitation,\n",
        "        FLAGS_COMMON.aspp_convs_filters,\n",
        "        FLAGS_COMMON.decoder_use_sum_merge,\n",
        "        FLAGS_COMMON.decoder_filters,\n",
        "        FLAGS_COMMON.decoder_output_is_logits,\n",
        "        FLAGS_COMMON.image_se_uses_qsigmoid,\n",
        "        label_weights,\n",
        "        'None',\n",
        "        FLAGS_COMMON.batch_norm_decay)\n",
        "\n",
        "  def __deepcopy__(self, memo):\n",
        "    return ModelOptions(copy.deepcopy(self.outputs_to_num_classes),\n",
        "                        self.crop_size,\n",
        "                        self.atrous_rates,\n",
        "                        self.output_stride,\n",
        "                        self.preprocessed_images_dtype)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTWH62FnSRxL",
        "colab_type": "text"
      },
      "source": [
        "# Params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oLGuBKnntCQ",
        "colab_type": "code",
        "outputId": "4569b73d-6cfa-447c-ec54-703ad6ca4658",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "params = {k: FLAGS_MAIN[k].value for k in FLAGS_MAIN}\n",
        "params"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'add_image_level_feature': True,\n",
              " 'alsologtostderr': False,\n",
              " 'aspp_convs_filters': 256,\n",
              " 'aspp_with_batch_norm': True,\n",
              " 'aspp_with_concat_projection': True,\n",
              " 'aspp_with_separable_conv': True,\n",
              " 'aspp_with_squeeze_and_excitation': False,\n",
              " 'atrous_rates': [6, 12, 18],\n",
              " 'batch_norm_decay': 0.9997,\n",
              " 'crop_size': [513, 513],\n",
              " 'dataset_dir': None,\n",
              " 'dataset_name': 'pascal_voc_seg',\n",
              " 'decoder_filters': 256,\n",
              " 'decoder_output_is_logits': False,\n",
              " 'decoder_output_stride': None,\n",
              " 'decoder_use_separable_conv': True,\n",
              " 'decoder_use_sum_merge': False,\n",
              " 'dense_prediction_cell_json': '',\n",
              " 'depth_multiplier': 1.0,\n",
              " 'divisible_by': None,\n",
              " 'eval_batch_size': 8,\n",
              " 'eval_split': 'val',\n",
              " 'eval_timeout': None,\n",
              " 'f': '/root/.local/share/jupyter/runtime/kernel-b76e1e98-9656-4827-b057-839224b8866f.json',\n",
              " 'fine_tune_batch_norm': True,\n",
              " 'gcp_project': None,\n",
              " 'image_pooling_crop_size': None,\n",
              " 'image_pooling_stride': ['1', '1'],\n",
              " 'image_pyramid': None,\n",
              " 'image_se_uses_qsigmoid': False,\n",
              " 'init_checkpoint': None,\n",
              " 'iterations_per_loop': 2000,\n",
              " 'keep_aspect_ratio': True,\n",
              " 'label_weights': None,\n",
              " 'learning_policy': 'poly',\n",
              " 'learning_power': 0.9,\n",
              " 'learning_rate': 0.01,\n",
              " 'learning_rate_decay': 0.97,\n",
              " 'log_dir': '',\n",
              " 'logits_kernel_size': 1,\n",
              " 'logtostderr': False,\n",
              " 'max_resize_value': None,\n",
              " 'max_scale_factor': 2.0,\n",
              " 'merge_method': 'max',\n",
              " 'min_resize_value': None,\n",
              " 'min_scale_factor': 0.5,\n",
              " 'mode': 'train_and_eval',\n",
              " 'model_dir': None,\n",
              " 'model_variant': 'mobilenet_v2',\n",
              " 'momentum': 0.9,\n",
              " 'multi_grid': None,\n",
              " 'nas_remove_os32_stride': False,\n",
              " 'nas_stem_output_num_conv_filters': 20,\n",
              " 'nas_use_classification_head': False,\n",
              " 'num_shards': 8,\n",
              " 'only_check_args': False,\n",
              " 'op_conversion_fallback_to_while_loop': False,\n",
              " 'optimizer': 'momentum',\n",
              " 'output_stride': 16,\n",
              " 'pdb_post_mortem': False,\n",
              " 'prediction_with_upsampled_logits': True,\n",
              " 'profile_file': None,\n",
              " 'resize_factor': None,\n",
              " 'run_with_pdb': False,\n",
              " 'run_with_profiling': False,\n",
              " 'save_checkpoints_steps': 2000,\n",
              " 'scale_factor_step_size': 0.25,\n",
              " 'showprefixforinfo': True,\n",
              " 'stderrthreshold': 'fatal',\n",
              " 'steps_per_eval': 2000,\n",
              " 'test_random_seed': 301,\n",
              " 'test_randomize_ordering_seed': None,\n",
              " 'test_srcdir': '',\n",
              " 'test_tmpdir': '/tmp/absl_testing',\n",
              " 'tpu': None,\n",
              " 'tpu_zone': None,\n",
              " 'train_batch_size': 64,\n",
              " 'train_split': 'train_aug',\n",
              " 'train_steps': 50000,\n",
              " 'upsample_logits': True,\n",
              " 'use_bfloat16': False,\n",
              " 'use_bounded_activation': False,\n",
              " 'use_cprofile_for_profiling': True,\n",
              " 'use_host_call': True,\n",
              " 'use_tpu': True,\n",
              " 'v': -1,\n",
              " 'verbosity': -1,\n",
              " 'weight_decay': 0.0001,\n",
              " 'xml_output_file': ''}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjQNoPqniE-D",
        "colab_type": "code",
        "outputId": "e3615da7-89af-4547-e8bb-0096fe6d3994",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# outputs_to_num_classes\n",
        "outputs_to_num_classes = {OUTPUT_TYPE: num_classes}\n",
        "outputs_to_num_classes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'semantic': 2}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDBr7EY2iQN9",
        "colab_type": "code",
        "outputId": "dd70262e-8bfa-4b53-be71-a11e009fc2ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# model_options\n",
        "model_options = ModelOptions(\n",
        "        outputs_to_num_classes, params['crop_size'], params['atrous_rates'],\n",
        "        params['output_stride'],\n",
        "        preprocessed_images_dtype=(tf.bfloat16 if params['use_bfloat16'] else tf.float32))\n",
        "model_options"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ModelOptions(outputs_to_num_classes={'semantic': 2}, crop_size=[513, 513], atrous_rates=[6, 12, 18], output_stride=16, preprocessed_images_dtype=tf.float32, merge_method='max', add_image_level_feature=True, image_pooling_crop_size=None, image_pooling_stride=[1, 1], aspp_with_batch_norm=True, aspp_with_separable_conv=True, multi_grid=None, decoder_output_stride=None, decoder_use_separable_conv=True, logits_kernel_size=1, model_variant='mobilenet_v2', depth_multiplier=1.0, divisible_by=None, prediction_with_upsampled_logits=True, dense_prediction_cell_config=None, nas_architecture_options={'nas_stem_output_num_conv_filters': 20, 'nas_use_classification_head': False, 'nas_remove_os32_stride': False}, use_bounded_activation=False, aspp_with_concat_projection=True, aspp_with_squeeze_and_excitation=False, aspp_convs_filters=256, decoder_use_sum_merge=False, decoder_filters=256, decoder_output_is_logits=False, image_se_uses_qsigmoid=False, label_weights=1.0, sync_batch_norm_method='None', batch_norm_decay=0.9997)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hnig5iP3a5s1",
        "colab_type": "code",
        "outputId": "1a8a7d6f-a576-432e-e137-524d71efef07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "params.update({'ignore_label': ignore_label,\n",
        "                 'model_options': model_options,\n",
        "                 'num_batches_per_epoch': num_batches_per_epoch,\n",
        "                 'num_classes': num_classes,\n",
        "                 'outputs_to_num_classes': outputs_to_num_classes})\n",
        "params"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'add_image_level_feature': True,\n",
              " 'alsologtostderr': False,\n",
              " 'aspp_convs_filters': 256,\n",
              " 'aspp_with_batch_norm': True,\n",
              " 'aspp_with_concat_projection': True,\n",
              " 'aspp_with_separable_conv': True,\n",
              " 'aspp_with_squeeze_and_excitation': False,\n",
              " 'atrous_rates': [6, 12, 18],\n",
              " 'batch_norm_decay': 0.9997,\n",
              " 'crop_size': [513, 513],\n",
              " 'dataset_dir': None,\n",
              " 'dataset_name': 'pascal_voc_seg',\n",
              " 'decoder_filters': 256,\n",
              " 'decoder_output_is_logits': False,\n",
              " 'decoder_output_stride': None,\n",
              " 'decoder_use_separable_conv': True,\n",
              " 'decoder_use_sum_merge': False,\n",
              " 'dense_prediction_cell_json': '',\n",
              " 'depth_multiplier': 1.0,\n",
              " 'divisible_by': None,\n",
              " 'eval_batch_size': 8,\n",
              " 'eval_split': 'val',\n",
              " 'eval_timeout': None,\n",
              " 'f': '/root/.local/share/jupyter/runtime/kernel-b76e1e98-9656-4827-b057-839224b8866f.json',\n",
              " 'fine_tune_batch_norm': True,\n",
              " 'gcp_project': None,\n",
              " 'ignore_label': 255,\n",
              " 'image_pooling_crop_size': None,\n",
              " 'image_pooling_stride': ['1', '1'],\n",
              " 'image_pyramid': None,\n",
              " 'image_se_uses_qsigmoid': False,\n",
              " 'init_checkpoint': None,\n",
              " 'iterations_per_loop': 2000,\n",
              " 'keep_aspect_ratio': True,\n",
              " 'label_weights': None,\n",
              " 'learning_policy': 'poly',\n",
              " 'learning_power': 0.9,\n",
              " 'learning_rate': 0.01,\n",
              " 'learning_rate_decay': 0.97,\n",
              " 'log_dir': '',\n",
              " 'logits_kernel_size': 1,\n",
              " 'logtostderr': False,\n",
              " 'max_resize_value': None,\n",
              " 'max_scale_factor': 2.0,\n",
              " 'merge_method': 'max',\n",
              " 'min_resize_value': None,\n",
              " 'min_scale_factor': 0.5,\n",
              " 'mode': 'train_and_eval',\n",
              " 'model_dir': None,\n",
              " 'model_options': ModelOptions(outputs_to_num_classes={'semantic': 2}, crop_size=[513, 513], atrous_rates=[6, 12, 18], output_stride=16, preprocessed_images_dtype=tf.float32, merge_method='max', add_image_level_feature=True, image_pooling_crop_size=None, image_pooling_stride=[1, 1], aspp_with_batch_norm=True, aspp_with_separable_conv=True, multi_grid=None, decoder_output_stride=None, decoder_use_separable_conv=True, logits_kernel_size=1, model_variant='mobilenet_v2', depth_multiplier=1.0, divisible_by=None, prediction_with_upsampled_logits=True, dense_prediction_cell_config=None, nas_architecture_options={'nas_stem_output_num_conv_filters': 20, 'nas_use_classification_head': False, 'nas_remove_os32_stride': False}, use_bounded_activation=False, aspp_with_concat_projection=True, aspp_with_squeeze_and_excitation=False, aspp_convs_filters=256, decoder_use_sum_merge=False, decoder_filters=256, decoder_output_is_logits=False, image_se_uses_qsigmoid=False, label_weights=1.0, sync_batch_norm_method='None', batch_norm_decay=0.9997),\n",
              " 'model_variant': 'mobilenet_v2',\n",
              " 'momentum': 0.9,\n",
              " 'multi_grid': None,\n",
              " 'nas_remove_os32_stride': False,\n",
              " 'nas_stem_output_num_conv_filters': 20,\n",
              " 'nas_use_classification_head': False,\n",
              " 'num_batches_per_epoch': 26.75,\n",
              " 'num_classes': 2,\n",
              " 'num_shards': 8,\n",
              " 'only_check_args': False,\n",
              " 'op_conversion_fallback_to_while_loop': False,\n",
              " 'optimizer': 'momentum',\n",
              " 'output_stride': 16,\n",
              " 'outputs_to_num_classes': {'semantic': 2},\n",
              " 'pdb_post_mortem': False,\n",
              " 'prediction_with_upsampled_logits': True,\n",
              " 'profile_file': None,\n",
              " 'resize_factor': None,\n",
              " 'run_with_pdb': False,\n",
              " 'run_with_profiling': False,\n",
              " 'save_checkpoints_steps': 2000,\n",
              " 'scale_factor_step_size': 0.25,\n",
              " 'showprefixforinfo': True,\n",
              " 'stderrthreshold': 'fatal',\n",
              " 'steps_per_eval': 2000,\n",
              " 'test_random_seed': 301,\n",
              " 'test_randomize_ordering_seed': None,\n",
              " 'test_srcdir': '',\n",
              " 'test_tmpdir': '/tmp/absl_testing',\n",
              " 'tpu': None,\n",
              " 'tpu_zone': None,\n",
              " 'train_batch_size': 64,\n",
              " 'train_split': 'train_aug',\n",
              " 'train_steps': 50000,\n",
              " 'upsample_logits': True,\n",
              " 'use_bfloat16': False,\n",
              " 'use_bounded_activation': False,\n",
              " 'use_cprofile_for_profiling': True,\n",
              " 'use_host_call': True,\n",
              " 'use_tpu': True,\n",
              " 'v': -1,\n",
              " 'verbosity': -1,\n",
              " 'weight_decay': 0.0001,\n",
              " 'xml_output_file': ''}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebEaD3LdpM9s",
        "colab_type": "text"
      },
      "source": [
        "# Deeplab estimator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT7JXHP6pZwt",
        "colab_type": "text"
      },
      "source": [
        "## model.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QEkgMZeqOfq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# multi_scale_logits\n",
        "def multi_scale_logits(images,\n",
        "                       model_options,\n",
        "                       image_pyramid,\n",
        "                       weight_decay=0.0001,\n",
        "                       is_training=False,\n",
        "                       fine_tune_batch_norm=False,\n",
        "                       nas_training_hyper_parameters=None):\n",
        "  \"\"\"Gets the logits for multi-scale inputs.\n",
        "\n",
        "  The returned logits are all downsampled (due to max-pooling layers)\n",
        "  for both training and evaluation.\n",
        "\n",
        "  Args:\n",
        "    images: A tensor of size [batch, height, width, channels].\n",
        "    model_options: A ModelOptions instance to configure models.\n",
        "    image_pyramid: Input image scales for multi-scale feature extraction.\n",
        "    weight_decay: The weight decay for model variables.\n",
        "    is_training: Is training or not.\n",
        "    fine_tune_batch_norm: Fine-tune the batch norm parameters or not.\n",
        "    nas_training_hyper_parameters: A dictionary storing hyper-parameters for\n",
        "      training nas models. Its keys are:\n",
        "      - `drop_path_keep_prob`: Probability to keep each path in the cell when\n",
        "        training.\n",
        "      - `total_training_steps`: Total training steps to help drop path\n",
        "        probability calculation.\n",
        "\n",
        "  Returns:\n",
        "    outputs_to_scales_to_logits: A map of maps from output_type (e.g.,\n",
        "      semantic prediction) to a dictionary of multi-scale logits names to\n",
        "      logits. For each output_type, the dictionary has keys which\n",
        "      correspond to the scales and values which correspond to the logits.\n",
        "      For example, if `scales` equals [1.0, 1.5], then the keys would\n",
        "      include 'merged_logits', 'logits_1.00' and 'logits_1.50'.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: If model_options doesn't specify crop_size and its\n",
        "      add_image_level_feature = True, since add_image_level_feature requires\n",
        "      crop_size information.\n",
        "  \"\"\"\n",
        "  # Setup default values.\n",
        "  if not image_pyramid:\n",
        "    image_pyramid = [1.0]\n",
        "  crop_height = (\n",
        "      model_options.crop_size[0]\n",
        "      if model_options.crop_size else tf.shape(images)[1])\n",
        "  crop_width = (\n",
        "      model_options.crop_size[1]\n",
        "      if model_options.crop_size else tf.shape(images)[2])\n",
        "  if model_options.image_pooling_crop_size:\n",
        "    image_pooling_crop_height = model_options.image_pooling_crop_size[0]\n",
        "    image_pooling_crop_width = model_options.image_pooling_crop_size[1]\n",
        "\n",
        "  # Compute the height, width for the output logits.\n",
        "  if model_options.decoder_output_stride:\n",
        "    logits_output_stride = min(model_options.decoder_output_stride)\n",
        "  else:\n",
        "    logits_output_stride = model_options.output_stride\n",
        "\n",
        "  logits_height = scale_dimension(\n",
        "      crop_height,\n",
        "      max(1.0, max(image_pyramid)) / logits_output_stride)\n",
        "  logits_width = scale_dimension(\n",
        "      crop_width,\n",
        "      max(1.0, max(image_pyramid)) / logits_output_stride)\n",
        "\n",
        "  # Compute the logits for each scale in the image pyramid.\n",
        "  outputs_to_scales_to_logits = {\n",
        "      k: {}\n",
        "      for k in model_options.outputs_to_num_classes\n",
        "  }\n",
        "\n",
        "  num_channels = images.get_shape().as_list()[-1]\n",
        "\n",
        "  for image_scale in image_pyramid:\n",
        "    if image_scale != 1.0:\n",
        "      scaled_height = scale_dimension(crop_height, image_scale)\n",
        "      scaled_width = scale_dimension(crop_width, image_scale)\n",
        "      scaled_crop_size = [scaled_height, scaled_width]\n",
        "      scaled_images = _resize_bilinear(images, scaled_crop_size, images.dtype)\n",
        "      if model_options.crop_size:\n",
        "        scaled_images.set_shape(\n",
        "            [None, scaled_height, scaled_width, num_channels])\n",
        "      # Adjust image_pooling_crop_size accordingly.\n",
        "      scaled_image_pooling_crop_size = None\n",
        "      if model_options.image_pooling_crop_size:\n",
        "        scaled_image_pooling_crop_size = [\n",
        "            scale_dimension(image_pooling_crop_height, image_scale),\n",
        "            scale_dimension(image_pooling_crop_width, image_scale)]\n",
        "    else:\n",
        "      scaled_crop_size = model_options.crop_size\n",
        "      scaled_images = images\n",
        "      scaled_image_pooling_crop_size = model_options.image_pooling_crop_size\n",
        "\n",
        "    updated_options = model_options._replace(\n",
        "        crop_size=scaled_crop_size,\n",
        "        image_pooling_crop_size=scaled_image_pooling_crop_size)\n",
        "    outputs_to_logits = _get_logits(\n",
        "        scaled_images,\n",
        "        updated_options,\n",
        "        weight_decay=weight_decay,\n",
        "        reuse=tf.AUTO_REUSE,\n",
        "        is_training=is_training,\n",
        "        fine_tune_batch_norm=fine_tune_batch_norm,\n",
        "        nas_training_hyper_parameters=nas_training_hyper_parameters)\n",
        "\n",
        "    # Resize the logits to have the same dimension before merging.\n",
        "    for output in sorted(outputs_to_logits):\n",
        "      outputs_to_logits[output] = _resize_bilinear(\n",
        "          outputs_to_logits[output], [logits_height, logits_width],\n",
        "          outputs_to_logits[output].dtype)\n",
        "\n",
        "    # Return when only one input scale.\n",
        "    if len(image_pyramid) == 1:\n",
        "      for output in sorted(model_options.outputs_to_num_classes):\n",
        "        outputs_to_scales_to_logits[output][\n",
        "            MERGED_LOGITS_SCOPE] = outputs_to_logits[output]\n",
        "      return outputs_to_scales_to_logits\n",
        "\n",
        "    # Save logits to the output map.\n",
        "    for output in sorted(model_options.outputs_to_num_classes):\n",
        "      outputs_to_scales_to_logits[output][\n",
        "          'logits_%.2f' % image_scale] = outputs_to_logits[output]\n",
        "\n",
        "  # Merge the logits from all the multi-scale inputs.\n",
        "  for output in sorted(model_options.outputs_to_num_classes):\n",
        "    # Concatenate the multi-scale logits for each output type.\n",
        "    all_logits = [\n",
        "        tf.expand_dims(logits, axis=4)\n",
        "        for logits in outputs_to_scales_to_logits[output].values()\n",
        "    ]\n",
        "    all_logits = tf.concat(all_logits, 4)\n",
        "    merge_fn = (\n",
        "        tf.reduce_max\n",
        "        if model_options.merge_method == 'max' else tf.reduce_mean)\n",
        "    outputs_to_scales_to_logits[output][MERGED_LOGITS_SCOPE] = merge_fn(\n",
        "        all_logits, axis=4)\n",
        "\n",
        "  return outputs_to_scales_to_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B63lJg3Gp_pB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# _build_network\n",
        "def _build_network(features, mode, params):\n",
        "  \"\"\"Builds the network for different values of params['use_bfloat16'].\"\"\"\n",
        "  if params['use_bfloat16']:\n",
        "    with bfloat16.bfloat16_scope():\n",
        "      outputs_to_scales_to_logits = multi_scale_logits(\n",
        "          features,\n",
        "          params['model_options'],\n",
        "          params['image_pyramid'],\n",
        "          weight_decay=0.0,\n",
        "          is_training=mode == tf.estimator.ModeKeys.TRAIN,\n",
        "          fine_tune_batch_norm=(\n",
        "              params['fine_tune_batch_norm']\n",
        "              if mode == tf.estimator.ModeKeys.TRAIN else False)\n",
        "      )\n",
        "    for level, output in outputs_to_scales_to_logits.iteritems():\n",
        "      for scale, logits in output.iteritems():\n",
        "        outputs_to_scales_to_logits[level][scale] = tf.cast(logits, tf.float32)\n",
        "  else:\n",
        "    outputs_to_scales_to_logits = multi_scale_logits(\n",
        "        features,\n",
        "        params['model_options'],\n",
        "        params['image_pyramid'],\n",
        "        weight_decay=params['weight_decay'],\n",
        "        is_training=mode == tf.estimator.ModeKeys.TRAIN,\n",
        "        fine_tune_batch_norm=(\n",
        "            params['fine_tune_batch_norm']\n",
        "            if mode == tf.estimator.ModeKeys.TRAIN else False)\n",
        "    )\n",
        "  return outputs_to_scales_to_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ih0A4i0mp61z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss_fn\n",
        "def loss_fn(features, labels, mode, params):\n",
        "  \"\"\"Computes label predictions and cross entropy loss against labels.\"\"\"\n",
        "  outputs_to_scales_to_logits = _build_network(features, mode, params)\n",
        "\n",
        "  for output, num_classes in params['outputs_to_num_classes'].items():\n",
        "    add_softmax_cross_entropy_loss_for_each_scale(\n",
        "        outputs_to_scales_to_logits[output],\n",
        "        labels,\n",
        "        num_classes,\n",
        "        ignore_label=params['ignore_label'],\n",
        "        loss_weight=1.0,\n",
        "        upsample_logits=params['upsample_logits'],\n",
        "        scope=output)\n",
        "\n",
        "  losses = tf.add_n(tf.losses.get_losses())\n",
        "  l2_loss = []\n",
        "  for v in tf.trainable_variables():\n",
        "    if 'BatchNorm' not in v.name and 'weights' in v.name:\n",
        "      l2_loss.append(tf.nn.l2_loss(v))\n",
        "  loss = losses + params['weight_decay'] * tf.add_n(l2_loss)\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3GFGiPgpZ-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\"\"\"Provide model_fn for TPUEstimator training and evaluation.\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.contrib.tpu.python.tpu import bfloat16\n",
        "from deeplab import common\n",
        "from deeplab.core import feature_extractor\n",
        "from deeplab.model import multi_scale_logits\n",
        "from deeplab.utils.train_utils import add_softmax_cross_entropy_loss_for_each_scale\n",
        "\n",
        "\n",
        "slim = tf.contrib.slim\n",
        "\n",
        "# Scope for the merged multi-scale logits.\n",
        "_MERGED_LOGITS_SCOPE = 'merged_logits'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _create_eval_metric(features, labels, params):\n",
        "  \"\"\"Creates eval_metric for model_fn.\"\"\"\n",
        "  outputs_to_scales_to_logits = _build_network(\n",
        "      features, tf.estimator.ModeKeys.EVAL, params)\n",
        "\n",
        "  semantic_merged_logits = (\n",
        "      outputs_to_scales_to_logits[common.OUTPUT_TYPE][_MERGED_LOGITS_SCOPE])\n",
        "\n",
        "  def metric_fn(semantic_merged_logits, labels):\n",
        "    \"\"\"Creates metric_fn for TPUEstimatorSpec.\"\"\"\n",
        "    logits = tf.image.resize_bilinear(\n",
        "        semantic_merged_logits, params['crop_size'], align_corners=True)\n",
        "    predictions_with_shape = tf.argmax(logits, 3, output_type=tf.int32)\n",
        "    predictions = tf.reshape(predictions_with_shape, shape=[-1])\n",
        "\n",
        "    labels = tf.reshape(labels, shape=[-1])\n",
        "    weights = tf.to_float(tf.not_equal(labels, params['ignore_label']))\n",
        "\n",
        "    # Set ignore_label regions to label 0, because metrics.mean_iou requires\n",
        "    # range of labels = [0, dataset.num_classes). Note the ignore_lable regions\n",
        "    # are not evaluated since the corresponding regions contain weights = 0.\n",
        "    labels = tf.where(\n",
        "        tf.equal(labels, params['ignore_label']), tf.zeros_like(labels), labels)\n",
        "\n",
        "    return {\n",
        "        'miou':\n",
        "            tf.metrics.mean_iou(\n",
        "                predictions, labels, params['num_classes'], weights=weights),\n",
        "    }\n",
        "\n",
        "  return metric_fn, [semantic_merged_logits, labels]\n",
        "\n",
        "\n",
        "def _get_optimizer(params, learning_rate):\n",
        "  \"\"\"Gets optimizer based on params.\"\"\"\n",
        "  if params['optimizer'] == 'momentum':\n",
        "    optimizer = tf.train.MomentumOptimizer(\n",
        "        learning_rate=learning_rate,\n",
        "        momentum=params['momentum'],\n",
        "        use_nesterov=True)\n",
        "  elif params['optimizer'] == 'sgd':\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "  elif params['optimizer'] == 'rmsprop':\n",
        "    optimizer = tf.train.RMSPropOptimizer(\n",
        "        learning_rate=learning_rate,\n",
        "        epsilon=params['rmsprop_epsilon'],\n",
        "        momentum=params['rmsprop_momentum'])\n",
        "  else:\n",
        "    raise KeyError('Unknown optimizer: %s' % params['optimizer'])\n",
        "\n",
        "  return optimizer\n",
        "\n",
        "\n",
        "def _get_learning_rate(params, global_step, num_batches_per_epoch):\n",
        "  \"\"\"Gets learning rate based on params.\"\"\"\n",
        "  learning_policy = params['learning_policy']\n",
        "  if learning_policy == 'poly':\n",
        "    learning_rate = tf.train.polynomial_decay(\n",
        "        params['learning_rate'],\n",
        "        global_step,\n",
        "        params['train_steps'],\n",
        "        end_learning_rate=0,\n",
        "        power=params['learning_power'])\n",
        "  elif learning_policy == 'step':\n",
        "    learning_rate = tf.train.exponential_decay(\n",
        "        params['learning_rate'],\n",
        "        global_step,\n",
        "        decay_rate=params['learning_rate_decay'],\n",
        "        decay_steps=num_batches_per_epoch,\n",
        "        staircase=True,\n",
        "    )\n",
        "  else:\n",
        "    raise KeyError('Unknown learning policy: %s' % learning_policy)\n",
        "\n",
        "  return learning_rate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MysUgB3p1hP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7ev0vntp1SI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_fn(features, labels, mode, params):\n",
        "  \"\"\"TPUEstimator compatible model function.\"\"\"\n",
        "  loss = loss_fn(features, labels, mode, params)\n",
        "\n",
        "  host_call = None\n",
        "  train_op = None\n",
        "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "    num_batches_per_epoch = params['num_batches_per_epoch']\n",
        "    global_step = tf.train.get_global_step()\n",
        "    current_epoch = tf.cast(global_step, tf.float32) / num_batches_per_epoch\n",
        "\n",
        "    learning_rate = _get_learning_rate(\n",
        "        params, global_step, num_batches_per_epoch)\n",
        "    optimizer = _get_optimizer(params, learning_rate)\n",
        "    if params['use_tpu']:\n",
        "      optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
        "\n",
        "    # Batch norm requires update_ops to be added as a train_op dependency.\n",
        "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    with tf.control_dependencies(update_ops):\n",
        "      train_op = optimizer.minimize(loss, tf.train.get_global_step())\n",
        "\n",
        "    if params['use_host_call']:\n",
        "      def host_call_fn(global_step, loss, learning_rate, current_epoch):\n",
        "        \"\"\"Training host call. Creates scalar summaries for training metrics.\n",
        "\n",
        "        This function is executed on the CPU and should not directly reference\n",
        "        any Tensors in the rest of the `model_fn`. To pass Tensors from the\n",
        "        model to the `metric_fn`, provide as part of the `host_call`. See\n",
        "        https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/TPUEstimatorSpec\n",
        "        for more information.\n",
        "\n",
        "        Arguments should match the list of `Tensor` objects passed as the second\n",
        "        element in the tuple passed to `host_call`.\n",
        "\n",
        "        Args:\n",
        "          global_step: `Tensor with shape `[batch, ]` for the global_step.\n",
        "          loss: `Tensor` with shape `[batch, ]` for the training loss.\n",
        "          learning_rate: `Tensor` with shape `[batch, ]` for the learning_rate.\n",
        "          current_epoch: `Tensor` with shape `[batch, ]` for the current_epoch.\n",
        "\n",
        "        Returns:\n",
        "          List of summary ops to run on the CPU host.\n",
        "        \"\"\"\n",
        "        # Outfeed supports int32 but global_step is expected to be int64.\n",
        "        global_step = tf.reduce_mean(global_step)\n",
        "        with (tf.contrib.summary.create_file_writer(\n",
        "            params['model_dir']).as_default()):\n",
        "          with tf.contrib.summary.always_record_summaries():\n",
        "            tf.contrib.summary.scalar(\n",
        "                'loss', tf.reduce_mean(loss), step=global_step)\n",
        "            tf.contrib.summary.scalar(\n",
        "                'learning_rate', tf.reduce_mean(learning_rate),\n",
        "                step=global_step)\n",
        "            tf.contrib.summary.scalar(\n",
        "                'current_epoch', tf.reduce_mean(current_epoch),\n",
        "                step=global_step)\n",
        "\n",
        "            return tf.contrib.summary.all_summary_ops()\n",
        "\n",
        "      # To log the loss, current learning rate, and epoch for Tensorboard, the\n",
        "      # summary op needs to be run on the host CPU via host_call. host_call\n",
        "      # expects [batch_size, ...] Tensors, thus reshape to introduce a batch\n",
        "      # dimension. These Tensors are implicitly concatenated to\n",
        "      # [params['batch_size']].\n",
        "      global_step_t = tf.reshape(global_step, [1])\n",
        "      loss_t = tf.reshape(loss, [1])\n",
        "      learning_rate_t = tf.reshape(learning_rate, [1])\n",
        "      current_epoch_t = tf.reshape(current_epoch, [1])\n",
        "\n",
        "      host_call = (host_call_fn,\n",
        "                   [global_step_t, loss_t, learning_rate_t, current_epoch_t])\n",
        "\n",
        "  eval_metrics = None\n",
        "  if mode == tf.estimator.ModeKeys.EVAL:\n",
        "    eval_metrics = _create_eval_metric(features, labels, params)\n",
        "\n",
        "  # Restore from checkpoint if available.\n",
        "  if params['init_checkpoint'] and mode == tf.estimator.ModeKeys.TRAIN:\n",
        "    tf.logging.info('Found an init checkpoint.')\n",
        "    model_variant = params['model_options'].model_variant\n",
        "    var_scope = '{}/'.format(feature_extractor.name_scope[model_variant])\n",
        "    def scaffold_fn():\n",
        "      \"\"\"Create Scaffold for initialization, etc.\"\"\"\n",
        "      tf.train.init_from_checkpoint(params['init_checkpoint'], {\n",
        "          var_scope: var_scope,\n",
        "      })\n",
        "      return tf.train.Scaffold()\n",
        "  else:\n",
        "    tf.logging.info('No init checkpoint found. Training from scratch.')\n",
        "    scaffold_fn = None\n",
        "  return tf.contrib.tpu.TPUEstimatorSpec(\n",
        "      mode=mode,\n",
        "      loss=loss,\n",
        "      train_op=train_op,\n",
        "      scaffold_fn=scaffold_fn,\n",
        "      host_call=host_call,\n",
        "      eval_metrics=eval_metrics,\n",
        "  )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f68xS3sIpyhE",
        "colab_type": "text"
      },
      "source": [
        "## deeplab_estimator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U60zHFl5pNXD",
        "colab_type": "code",
        "outputId": "0966f3c2-e28d-4684-bc0f-3fd248f2fdd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "deeplab_estimator = tf.contrib.tpu.TPUEstimator(\n",
        "      use_tpu=FLAGS_MAIN.use_tpu,\n",
        "      model_fn=model.model_fn,\n",
        "      config=config,\n",
        "      train_batch_size=FLAGS_MAIN.train_batch_size,\n",
        "      eval_batch_size=FLAGS_MAIN.eval_batch_size,\n",
        "      params=params)\n",
        "deeplab_estimator"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-f428a68afd25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m deeplab_estimator = tf.contrib.tpu.TPUEstimator(\n\u001b[1;32m      2\u001b[0m       \u001b[0muse_tpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLAGS_MAIN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_tpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m       \u001b[0mmodel_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m       \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mtrain_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLAGS_MAIN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUKMlRn9px7K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}